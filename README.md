# ClarityAI
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)

ClarityAI is a Python package designed to empower machine learning practitioners with a range of interpretability methods to enhance the transparency and explainability of their CNN models. Currently, ClarityAI can calculate attention and saliency maps.

## Installation + Usage
To install and use ClarityAI, please refer to our [wiki](https://github.com/JasmineZhangxyz/clarityai-pypkg/wiki) for instructions.

## Features
* Documentation for attention map generation can be found [here](https://github.com/JasmineZhangxyz/clarityai-pypkg/wiki/Attention-Maps)
* Documentation for saliency map generation can be found [here](https://github.com/JasmineZhangxyz/clarityai-pypkg/wiki/Saliency-Maps)

## Limitations
ClarityAI is designed to help users quickly integrate interpretability methods into their personal projects. However, ClarityAI is just a tool meant to help users - not replace users' own judgments on interpretability and ethical use cases of their ML models.

Please also note that ClarityAI is a package created for fun/educational purposes! There exist several popular interpretability libraries available in the Python ecosystem, which are much better designed and maintained, such as [SHAP](https://shap.readthedocs.io/en/latest/), [LIME](https://github.com/marcotcr/lime), [Yellowbrick](https://www.scikit-yb.org/en/latest/), and [InterpretML](https://github.com/interpretml/interpret).
